{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_ssd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bdd16089fdc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mviz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisdom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisdom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mssd_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_ssd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssd_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_ssd' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "from data import v2, v1, AnnotationTransform, VOCDetection, detection_collate, VOCroot, VOC_CLASSES\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "#from ssd import build_ssd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Single Shot MultiBox Detector Training')\n",
    "parser.add_argument('--version', default='v2', help='conv11_2(v2) or pool6(v1) as last layer')\n",
    "parser.add_argument('--basenet', default='vgg16_reducedfc.pth', help='pretrained base model')\n",
    "parser.add_argument('--jaccard_threshold', default=0.5, type=float, help='Min Jaccard index for matching')\n",
    "parser.add_argument('--batch_size', default=16, type=int, help='Batch size for training')\n",
    "parser.add_argument('--resume', default=None, type=str, help='Resume from checkpoint')\n",
    "parser.add_argument('--num_workers', default=2, type=int, help='Number of workers used in dataloading')\n",
    "parser.add_argument('--iterations', default=120000, type=int, help='Number of training iterations')\n",
    "parser.add_argument('--start_iter', default=0, type=int, help='Begin counting iterations starting from this value (should be used with resume)')\n",
    "parser.add_argument('--cuda', default=True, type=str2bool, help='Use cuda to train model')\n",
    "parser.add_argument('--lr', '--learning-rate', default=1e-3, type=float, help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--weight_decay', default=5e-4, type=float, help='Weight decay for SGD')\n",
    "parser.add_argument('--gamma', default=0.1, type=float, help='Gamma update for SGD')\n",
    "parser.add_argument('--log_iters', default=True, type=bool, help='Print the loss at each iteration')\n",
    "parser.add_argument('--visdom', default=False, type=str2bool, help='Use visdom to for loss visualization')\n",
    "parser.add_argument('--send_images_to_visdom', type=str2bool, default=False, help='Sample a random image from each 10th batch, send it to visdom after augmentations step')\n",
    "parser.add_argument('--save_folder', default='weights/', help='Location to save checkpoint models')\n",
    "parser.add_argument('--voc_root', default=VOCroot, help='Location of VOC root directory')\n",
    "#args = parser.parse_args(['--voc_root','/home/amir/code/jacky/'])\n",
    "args = parser.parse_args(['--voc_root','/home/amir/data/voc/VOCdevkit/'])\n",
    "\n",
    "if args.cuda and torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "cfg = (v1, v2)[args.version == 'v2']\n",
    "\n",
    "if not os.path.exists(args.save_folder):\n",
    "    os.mkdir(args.save_folder)\n",
    "\n",
    "#train_sets = [('2007', 'trainval'), ('2012', 'trainval')]\n",
    "train_sets = [('2007', 'trainval')]\n",
    "# train_sets = 'train'\n",
    "ssd_dim = 300  # only support 300 now\n",
    "means = (104, 117, 123)  # only support voc now\n",
    "num_classes = len(VOC_CLASSES) + 1\n",
    "batch_size = args.batch_size\n",
    "accum_batch_size = 32\n",
    "iter_size = accum_batch_size / batch_size\n",
    "max_iter = 120000\n",
    "weight_decay = 0.0005\n",
    "stepvalues = (80000, 100000, 120000)\n",
    "gamma = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "if args.visdom:\n",
    "    import visdom\n",
    "    viz = visdom.Visdom()\n",
    "\n",
    "ssd_net = build_ssd('train', 300, num_classes)\n",
    "net = ssd_net\n",
    "\n",
    "if args.cuda:\n",
    "    net = torch.nn.DataParallel(ssd_net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if args.resume:\n",
    "    print('Resuming training, loading {}...'.format(args.resume))\n",
    "    ssd_net.load_weights(args.resume)\n",
    "else:\n",
    "    vgg_weights = torch.load(args.save_folder + args.basenet)\n",
    "    print('Loading base network...')\n",
    "    ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "if args.cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "\n",
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "if not args.resume:\n",
    "    print('Initializing weights...')\n",
    "    # initialize newly added layers' weights with xavier method\n",
    "    ssd_net.extras.apply(weights_init)\n",
    "    ssd_net.loc.apply(weights_init)\n",
    "    ssd_net.conf.apply(weights_init)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr,\n",
    "                      momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "criterion = MultiBoxLoss(num_classes, 0.5, True, 0, True, 3, 0.5, False, args.cuda)\n",
    "\n",
    "\n",
    "def train():\n",
    "    net.train()\n",
    "    # loss counters\n",
    "    loc_loss = 0  # epoch\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print('Loading Dataset...')\n",
    "\n",
    "    dataset = VOCDetection(args.voc_root, train_sets, SSDAugmentation(\n",
    "        ssd_dim, means), AnnotationTransform())\n",
    "\n",
    "    epoch_size = len(dataset) // args.batch_size\n",
    "    print('Training SSD on', dataset.name)\n",
    "    step_index = 0\n",
    "    if args.visdom:\n",
    "        # initialize visdom loss plot\n",
    "        lot = viz.line(\n",
    "            X=torch.zeros((1,)).cpu(),\n",
    "            Y=torch.zeros((1, 3)).cpu(),\n",
    "            opts=dict(\n",
    "                xlabel='Iteration',\n",
    "                ylabel='Loss',\n",
    "                title='Current SSD Training Loss',\n",
    "                legend=['Loc Loss', 'Conf Loss', 'Loss']\n",
    "            )\n",
    "        )\n",
    "        epoch_lot = viz.line(\n",
    "            X=torch.zeros((1,)).cpu(),\n",
    "            Y=torch.zeros((1, 3)).cpu(),\n",
    "            opts=dict(\n",
    "                xlabel='Epoch',\n",
    "                ylabel='Loss',\n",
    "                title='Epoch SSD Training Loss',\n",
    "                legend=['Loc Loss', 'Conf Loss', 'Loss']\n",
    "            )\n",
    "        )\n",
    "    batch_iterator = None\n",
    "    data_loader = data.DataLoader(dataset, batch_size, num_workers=args.num_workers,\n",
    "                                  shuffle=True, collate_fn=detection_collate, pin_memory=True)\n",
    "    for iteration in range(args.start_iter, max_iter):\n",
    "        if (not batch_iterator) or (iteration % epoch_size == 0):\n",
    "            # create batch iterator\n",
    "            batch_iterator = iter(data_loader)\n",
    "        if iteration in stepvalues:\n",
    "            step_index += 1\n",
    "            adjust_learning_rate(optimizer, args.gamma, step_index)\n",
    "            if args.visdom:\n",
    "                viz.line(\n",
    "                    X=torch.ones((1, 3)).cpu() * epoch,\n",
    "                    Y=torch.Tensor([loc_loss, conf_loss,\n",
    "                        loc_loss + conf_loss]).unsqueeze(0).cpu() / epoch_size,\n",
    "                    win=epoch_lot,\n",
    "                    update='append'\n",
    "                )\n",
    "            # reset epoch loss counters\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "            epoch += 1\n",
    "\n",
    "        # load train data\n",
    "        images, targets = next(batch_iterator)\n",
    "\n",
    "        if args.cuda:\n",
    "            images = Variable(images.cuda())\n",
    "            targets = [Variable(anno.cuda(), volatile=True) for anno in targets]\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            targets = [Variable(anno, volatile=True) for anno in targets]\n",
    "        # forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.data[0]\n",
    "        conf_loss += loss_c.data[0]\n",
    "        if iteration % 10 == 0:\n",
    "            print('Timer: %.4f sec.' % (t1 - t0))\n",
    "            #print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data[0]), end=' ')\n",
    "            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data[0]))\n",
    "            if args.visdom and args.send_images_to_visdom:\n",
    "                random_batch_index = np.random.randint(images.size(0))\n",
    "                viz.image(images.data[random_batch_index].cpu().numpy())\n",
    "        if args.visdom:\n",
    "            viz.line(\n",
    "                X=torch.ones((1, 3)).cpu() * iteration,\n",
    "                Y=torch.Tensor([loss_l.data[0], loss_c.data[0],\n",
    "                    loss_l.data[0] + loss_c.data[0]]).unsqueeze(0).cpu(),\n",
    "                win=lot,\n",
    "                update='append'\n",
    "            )\n",
    "            # hacky fencepost solution for 0th epoch plot\n",
    "            if iteration == 0:\n",
    "                viz.line(\n",
    "                    X=torch.zeros((1, 3)).cpu(),\n",
    "                    Y=torch.Tensor([loc_loss, conf_loss,\n",
    "                        loc_loss + conf_loss]).unsqueeze(0).cpu(),\n",
    "                    win=epoch_lot,\n",
    "                    update=True\n",
    "                )\n",
    "        if iteration % 5000 == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            torch.save(ssd_net.state_dict(), 'weights/ssd300_0712_' +\n",
    "                       repr(iteration) + '.pth')\n",
    "    torch.save(ssd_net.state_dict(), args.save_folder + '' + args.version + '.pth')\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    lr = args.lr * (gamma ** (step))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = VOCDetection(args.voc_root, train_sets, SSDAugmentation(\n",
    "        ssd_dim, means), AnnotationTransform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-60a934a571e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                               shuffle=True, collate_fn=detection_collate, pin_memory=True)\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mbatch_iterator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# create batch iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mbatch_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_size' is not defined"
     ]
    }
   ],
   "source": [
    "batch_iterator = None\n",
    "data_loader = data.DataLoader(dataset, batch_size, num_workers=args.num_workers,\n",
    "                              shuffle=True, collate_fn=detection_collate, pin_memory=True)\n",
    "for iteration in range(args.start_iter, max_iter):\n",
    "    if (not batch_iterator) or (iteration % epoch_size == 0):\n",
    "        # create batch iterator\n",
    "        batch_iterator = iter(data_loader)\n",
    "    if iteration in stepvalues:\n",
    "        step_index += 1\n",
    "        adjust_learning_rate(optimizer, args.gamma, step_index)\n",
    "        if args.visdom:\n",
    "            viz.line(\n",
    "                X=torch.ones((1, 3)).cpu() * epoch,\n",
    "                Y=torch.Tensor([loc_loss, conf_loss,\n",
    "                    loc_loss + conf_loss]).unsqueeze(0).cpu() / epoch_size,\n",
    "                win=epoch_lot,\n",
    "                update='append'\n",
    "            )\n",
    "        # reset epoch loss counters\n",
    "        loc_loss = 0\n",
    "        conf_loss = 0\n",
    "        epoch += 1\n",
    "\n",
    "    # load train data\n",
    "    images, targets = next(batch_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_sets = [('2007', 'trainval')]\n",
    "targetTransform =AnnotationTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#root = '/home/amir/code/jacky/'\n",
    "root = '/home/amir/data/voc/VOCdevkit/'\n",
    "image_set = 'trainval.txt'\n",
    "#self.transform = transform\n",
    "#self.target_transform = target_transform\n",
    "name = 'VOC2007'\n",
    "_annopath = os.path.join('%s', 'Annotations', '%s.xml')\n",
    "_imgpath = os.path.join('%s', 'JPEGImages', '%s.jpg')\n",
    "ids = list()\n",
    "for (year, name) in image_sets:\n",
    "    rootpath = os.path.join(root, 'VOC' + year)\n",
    "    for line in open(os.path.join(rootpath, 'ImageSets', 'Main', name + '.txt')):\n",
    "        ids.append((rootpath, line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/amir/data/voc/VOCdevkit/VOC2007', '000005')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300\n",
      "48 --> 0\n",
      "45 --> 0\n",
      "143 --> 0\n",
      "140 --> 0\n",
      "/home/amir/code/jacky/VOC2007/Annotations/1.xml\n",
      "/home/amir/code/jacky/VOC2007/JPEGImages/1.jpg\n",
      "[[0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "class AnnotationTransform(object):\n",
    "    \"\"\"Transforms a VOC annotation into a Tensor of bbox coords and label index\n",
    "    Initilized with a dictionary lookup of classnames to indexes\n",
    "\n",
    "    Arguments:\n",
    "        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n",
    "            (default: alphabetic indexing of VOC's 20 classes)\n",
    "        keep_difficult (bool, optional): keep difficult instances or not\n",
    "            (default: False)\n",
    "        height (int): height\n",
    "        width (int): width\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_to_ind=None, keep_difficult=False):\n",
    "        self.class_to_ind = class_to_ind or dict(\n",
    "            zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n",
    "        self.keep_difficult = keep_difficult\n",
    "\n",
    "    def __call__(self, target, width, height):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            target (annotation) : the target annotation to be made usable\n",
    "                will be an ET.Element\n",
    "        Returns:\n",
    "            a list containing lists of bounding boxes  [bbox coords, class name]\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for obj in target.iter('object'):\n",
    "            difficult = int(obj.find('difficult').text) == 1\n",
    "            if not self.keep_difficult and difficult:\n",
    "                continue\n",
    "            name = obj.find('name').text.lower().strip()\n",
    "            bbox = obj.find('bndbox')\n",
    "            print width,height\n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "            bndbox = []\n",
    "            for i, pt in enumerate(pts):\n",
    "                cur_pt = int(bbox.find(pt).text) - 1\n",
    "                print cur_pt,'-->',\n",
    "                # scale height or width\n",
    "                cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n",
    "                print cur_pt\n",
    "                bndbox.append(cur_pt)\n",
    "            label_idx = self.class_to_ind[name]\n",
    "            bndbox.append(label_idx)\n",
    "            res += [bndbox]  # [xmin, ymin, xmax, ymax, label_ind]\n",
    "            # img_id = target.find('filename').text[:-4]\n",
    "\n",
    "        return res  # [[xmin, ymin, xmax, ymax, label_ind], ... ]\n",
    "\n",
    "targetTransform =AnnotationTransform()\n",
    "\n",
    "img_id = ids[0]\n",
    "import cv2\n",
    "\n",
    "root = '/home/amir/code/jacky/'\n",
    "#root = '/home/amir/data/voc/VOCdevkit/'\n",
    "image_set = 'trainval.txt'\n",
    "#self.transform = transform\n",
    "#self.target_transform = target_transform\n",
    "name = 'VOC2007'\n",
    "_annopath = os.path.join('%s', 'Annotations', '%s.xml')\n",
    "_imgpath = os.path.join('%s', 'JPEGImages', '%s.jpg')\n",
    "ids = list()\n",
    "for (year, name) in image_sets:\n",
    "    rootpath = os.path.join(root, 'VOC' + year)\n",
    "    for line in open(os.path.join(rootpath, 'ImageSets', 'Main', name + '.txt')):\n",
    "        ids.append((rootpath, line.strip()))\n",
    "\n",
    "img_id = ids[0]\n",
    "import xml.etree.cElementTree as ET\n",
    "target = ET.parse(_annopath % img_id).getroot()\n",
    "img = cv2.imread(_imgpath % img_id)\n",
    "height, width, channels = img.shape\n",
    "target1= targetTransform(target, width, height)\n",
    "print _annopath % img_id\n",
    "print _imgpath % img_id\n",
    "print target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%s/Annotations/%s.xml'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element 'name' at 0x7fe763209c90>,\n",
       " <Element 'pose' at 0x7fe763209c30>,\n",
       " <Element 'truncated' at 0x7fe763209bd0>,\n",
       " <Element 'difficult' at 0x7fe763209ba0>,\n",
       " <Element 'bndbox' at 0x7fe763209b70>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(target.getchildren())[-1].getchildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amir/data/voc/VOCdevkit/VOC2007/Annotations/000005.xml'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_annopath % img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 8], [0, 0, 0, 0, 8], [0, 0, 0, 0, 8]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/amir/code/jacky/VOC2007', '25')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() takes exactly 4 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-277dabaf65e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtargetTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __call__() takes exactly 4 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "targetTransform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  0  0  0  0  2\n",
       " [torch.FloatTensor of size 1x5], \n",
       "  0  0  0  0  3\n",
       " [torch.FloatTensor of size 1x5], \n",
       "     0     0     0     0     4\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0    10\n",
       "     0     0     0     0     8\n",
       "     0     0     0     0     8\n",
       " [torch.FloatTensor of size 13x5], \n",
       "  1  0  1  0  2\n",
       " [torch.FloatTensor of size 1x5], \n",
       "   1   0   1   0   6\n",
       "   1   0   1   0   6\n",
       "   1   0   1   0   6\n",
       "   1   0   1   0  14\n",
       "   1   0   1   0  13\n",
       " [torch.FloatTensor of size 5x5], \n",
       "     0     0     0     0    15\n",
       "     0     0     0     0    15\n",
       "     0     0     0     0    15\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0    14\n",
       "     0     0     0     0     7\n",
       " [torch.FloatTensor of size 7x5], \n",
       "   0   0   0   0  19\n",
       "   0   0   0   0  19\n",
       "   0   0   0   0   8\n",
       " [torch.FloatTensor of size 3x5], \n",
       "  0  0  0  0  7\n",
       "  0  0  0  0  6\n",
       " [torch.FloatTensor of size 2x5], \n",
       "   0   0   0   0  18\n",
       "   0   0   0   0   6\n",
       " [torch.FloatTensor of size 2x5], \n",
       "   0   0   0   0  11\n",
       "   0   0   0   0   7\n",
       "   0   0   0   0  17\n",
       " [torch.FloatTensor of size 3x5], \n",
       "     1     0     1     0    14\n",
       "     1     0     1     0    14\n",
       "     1     0     1     0    14\n",
       "     1     0     1     0    14\n",
       "     1     0     1     0    14\n",
       "     1     0     1     0    14\n",
       "     1     0     1     0    10\n",
       " [torch.FloatTensor of size 7x5], \n",
       "   1   0   1   0  14\n",
       "   1   0   1   0   2\n",
       " [torch.FloatTensor of size 2x5], \n",
       "  0  0  0  0  2\n",
       " [torch.FloatTensor of size 1x5], \n",
       "   0   0   0   0  18\n",
       " [torch.FloatTensor of size 1x5], \n",
       "   0   0   0   0  14\n",
       " [torch.FloatTensor of size 1x5], \n",
       "   1   0   1   0  11\n",
       "   1   0   1   0  14\n",
       "   1   0   1   0   3\n",
       " [torch.FloatTensor of size 3x5]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[('/home/amir/data/voc/VOCdevkit/VOC2007', '000005'),\n",
    " ('/home/amir/data/voc/VOCdevkit/VOC2007', '000007'),\n",
    " ('/home/amir/data/voc/VOCdevkit/VOC2007', '000009'),\n",
    " ('/home/amir/data/voc/VOCdevkit/VOC2007', '000012'),\n",
    " ('/home/amir/data/voc/VOCdevkit/VOC2007', '000016'),\n",
    " ('/home/amir/data/voc/VOCdevkit/VOC2007', '000017'),\n",
    " ('/home/amir/data/voc/VOCdevkit/VOC2007', '000019'),\n",
    " ('/home/amir/data/voc/VOCdevkit/VOC2007', '000020'),"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:universe]",
   "language": "python",
   "name": "conda-env-universe-py"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
